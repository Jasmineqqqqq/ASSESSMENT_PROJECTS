{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "036aec11",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Decision Tree(mainlyuse for classification moddel)\n",
    "#Decision tree algorithm is one of the most versatile algorithms in machine learning which can perform both \n",
    "#classification and regression analysis. It is very\n",
    "#powerful and works great with complex datasets. Apart from that, it is very easy to understand and read. \n",
    "#That makes it more popular to use. When coupled\n",
    "#with ensemble techniques - which we will learn very soon- it performs even better. \n",
    "#As the name suggests, this algorithm works by dividing the whole dataset\n",
    "#into a tree-like structure based on some rules and conditions and then gives prediction based on those conditions. \n",
    "#Let's understand the approach to decision tree with a basic scenario. \n",
    "#Suppose it's Friday night and you are not able to decide if you should go out or stay at home. \n",
    "#Let the decision tree decide it for you."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb2450dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Entropy snfd gini is used to determine best root node or feature\n",
    "#Classification Trees\n",
    "\n",
    "#Regression trees are used for quantitative data. In the case of qualitative data or categorical data, \n",
    "#we use classification trees. In regression trees, we split the\n",
    "#nodes based on RSS criteria, but in classification, it is done using classification error rate, Gini impurity and entropy.\n",
    "#Let's understand these terms in detail.\n",
    "\n",
    "#Entropy: Extropy is the measure of randomness in the data. In other words, it gives the impurity present in the dataset.\n",
    "#When we split our nodes into two regions and put different observations in both the regions, the main goal is to reduce the \n",
    "#entropy i.e. reduce the randomness in the region and divide our data cleanly than it was in the previous node. \n",
    "#If splitting the node doesn't lead into entropy\n",
    "#reduction, we try to split based on a different condition or we stop.\n",
    "#A region is clean (low entropy) when it contains data with the same labels and random if there is a mixture of labels present\n",
    "#(high entropy). Let's suppose there are m' observations and we need to classify them into categories 1 and 2. \n",
    "#let's say that category 1 has n' observations and category 2 has m-n' observations.\n",
    "#p= n/m and q = m-n/m = 1-p\n",
    "\n",
    "#then, entropy for the given set is:(best impact feature to label is selected as root node)\n",
    "#Entropy\n",
    "#Extropy is the measure of randomness in the data. In other words, it gives the impurity present in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d792dc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Where, T is the parent node before split and X is the split node from T.\n",
    "#Gain(T.X)=Entropy(T)-Entropy(T.X)\n",
    "#Information Gain\n",
    "#Information gain calculates the decrease in entropy after splitting a node. \n",
    "#It is the difference between entropies before and after the split. The more the\n",
    "#information gain the more entropy is removed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93b599cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#When all the observations belong to category 1, then p = 1 and all observations belong to category 2, then p =0,\n",
    "#int both cases E =0, as there is no randomness in the categories. \n",
    "#If half of the observations are in category 1 and another half in category 2, then p =1/2 and q =1\n",
    "#2, and the entropy is maximum, E =1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d12b25d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Gini Impurity(lowest impurity isbest gini)\n",
    "#According to wikipedia,\n",
    "#Gini impurity is a measure of how often a randomly chosen element from the set would be incorrectly labelled if it was randomly\n",
    "#labelled according to the distribution of labels in the subset.' \n",
    "#It is calculated by multiplying the probability that a given observation is classified into the correct\n",
    "#class and sum of all the probabilities when that particular observation is classified into the wrong class.\n",
    "#Ginni impurity value lies between 0 and 1, 0 being no impurity and 1 denoting random distribution.\n",
    "#The node for which the Ginni impurity is least is selected as the root node to split.\n",
    "# tree which is splitted on basis of ginni impurity value looks like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f96fe3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#classification use entropy and regression use GINI\n",
    "#ALGORITHMS IN DECISION TREE\n",
    "#ID3 (Iterative Dichotomiser): It is one of the algorithms used to construct decision tree for classification. \n",
    "#It uses Information gain as the criteria for finding\n",
    "#the root nodes and splitting them. It only accepts categorical attributes.\n",
    "\n",
    "#C4.5: It is an extension of ID3 algorithm, and better than ID3 as it deals both continuous and discreet values. \n",
    "#It is also used for classfication purposes.\n",
    "\n",
    "#Classfication and Regression Algorithm(CART): It is the most popular algorithm used for constructing decison trees. \n",
    "#It uses ginni impurity as the default\n",
    "#calculation for selecting root nodes, however one can use \"problems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e593f78c",
   "metadata": {},
   "outputs": [],
   "source": [
    "Advantages of Decision Tree:\n",
    "It can be used for both Regression and Classification problems.\n",
    "Decision Trees are very easy to grasp as the rules of splitting is clearly metioned\n",
    "Complex decision tree models are very simple when visualized\n",
    "It can be understood just by visualising.\n",
    "Scaling and normalization are not needed\n",
    "\n",
    "Disadvantages of Decision Tree:\n",
    "A small change in data can cause instability in the model because of the greedy approach.\n",
    "Probability of overfitting is very high for Decision Trees.\n",
    "It takes more time to train a decision tree model than other classification algorithms.\n",
    "is clearly mentioned.\n",
    ".\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff21dc72",
   "metadata": {},
   "outputs": [],
   "source": [
    "Implementation in Python\n",
    "we will use Sklearn module to implement decision tree algorithm. Sklearn uses CART\n",
    "classification and Regression trees) algorithm and by default it uses\n",
    "Gini impurity as a criteria to split the nodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59a2cb8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "Implementation in Python\n",
    "we will use Sklearn module to implement decision tree algorithm. Sklearn uses CART (classification and Regression trees) \n",
    "algorithm and by default it uses\n",
    "Gini impurity as a criteria to split the nodes.\n",
    "There are other algorithms like ID3, C4.5, Chi-square etc.\n",
    "We will see the use of CART in following implementation."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
