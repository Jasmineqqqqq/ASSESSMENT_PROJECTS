{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3651d5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "................Logistic Regression\n",
    "Logistic Regression is one of the most fundamental algorithms for classification in the Machine Learning world.\n",
    "But before proceeding with the algorithm, let's first discuss the lifecycle of any machine learning model. This diagram explains the creation of a Machine\n",
    "Learning model from scratch and then taking the same model further with hyperparameter tuning to increase its accuracy, deciding the deployment strategies\n",
    "for that model and once deployed setting up the logging and monitoring..................."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "580f8864",
   "metadata": {},
   "outputs": [],
   "source": [
    "............For linear regression, the model is defined by:\n",
    "i)\n",
    "and for logistic regression, we calculate probability, i.e. y is the probability of a given variable x belonging\n",
    "to a certain class. Thus it is obvious that the value of y should lie between 0 and 1\n",
    "But, when we use equation(i) to calculate probability, we would get values less than 0 as well as greater than 1.\n",
    "That doesn't make any sense. So, we need to\n",
    "use such an equation which always gives values between 0 and 1, as we desire while calculating the probability.\n",
    "\n",
    "Sigmoid function(it categorises data, based on threshold-if below 50 % is positive, then it will be 1, and above 50, ie 51 is 0)\n",
    "1) The sigmoid function's range is bounded between 0 and 1. Thus it's useful in calculating the probability for the Logistic function. 2) It's derivative is easy to\n",
    "calculate than other functions which is useful during gradient descent calculation\n",
    "3) It is a simple way of introducing non\n",
    "linearity to the model.\n",
    "Although there are other functions as well, which can be used, but sigmoid is the most common function used for logistic regression.\n",
    "The logistic function is given as:............................."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40bb2f61",
   "metadata": {},
   "outputs": [],
   "source": [
    "Evaluation of a Classification Model\n",
    "In machine learning, once we have a result of the classification problem\n",
    "how do we measure how accurate our classification is? For a regression problem, we\n",
    "have different metrics like R Squared score, Mean Squared Error etc. what are the metrics to measure the credibility\n",
    "of a classification model?\n",
    "Metrics In a regression problem, the accuracy is generally measured in terms of the difference in the actual values\n",
    "and the predicted values. In a classification problem, the credibility of the model is measured using the confusion matrix\n",
    "generated, i.e., how accurately the true positives and true negatives were\n",
    "Accuracy\n",
    "Recall\n",
    "Precision\n",
    "F1 Score\n",
    "Specificity\n",
    "AUC( Area Under the Curve)\n",
    "ROC(Receiver Operator Characteristic)\n",
    "predicted. The different metrics used for this purpose are:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0fc69ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "True Positive(TP): A result that was predicted as positive by the classification model and also is positive\n",
    "True Negative(TN): A result that was predicted as negative by the classification model and also is negative\n",
    "False Positive(FP): A result that was predicted as positive by the classification model but actually is negative\n",
    "False Negative (FN): A result that was predicted as negative by the classification model but actually is positive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "301768e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy, here the correct classifications come under true scenarios, negative or positive\n",
    "The mathematical formula is :\n",
    "(TP+TN)/(TP+TN+FP+FN)\n",
    "Or, can be said that it's defined as the total number of correct classifications divided by the total number of classifications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9565d2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "Recall or Sensitivity\n",
    "recall takes all positive scenarios ie true postive and false negative\n",
    "it recalls or tries ro remember hw many are actually correct or positive: TP/TP+FN\n",
    "if it identifies only 50 as cancer(TP), but failed to identify 200 patients (FN), u keep in formula and u get that model\n",
    "only recalls 20% of cancer patients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6cb2013a",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (3099266480.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Input \u001b[1;32mIn [3]\u001b[1;36m\u001b[0m\n\u001b[1;33m    .....Precision\u001b[0m\n\u001b[1;37m        ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    ".....Precision\n",
    "Precision is a measure of amongst all the positive predictions, how many of them were actually positive. Mathematically,\n",
    "TP/(TP+FP)\n",
    "Type 1 error is FP | type 2 error is FN\n",
    "Let's suppose in the previous example\n",
    "the model identified 50 people as cancer patients\n",
    "TP) but also raised a false alarm for 100 patients (FP). Hence,.........\n",
    "precision = 50/(50+100)\n",
    "#-0.33 (The model only has a precision of 33%)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "030c9abf",
   "metadata": {},
   "outputs": [],
   "source": [
    "when it comes to business usually precision, when it comes to disease where evry percentage matters recall,\n",
    "when no loss/gain we use accurqcy\n",
    "\n",
    "when u dont knpw what to do, which one to select, u use a trade off\n",
    "\n",
    "This tradeoff is F1 score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a99e1224",
   "metadata": {},
   "outputs": [],
   "source": [
    "F1 Score\n",
    "From the previous examples, it is clear that we need a metric that considers both Precision and Recall for evaluating a model. One such metric is the F1 score.\n",
    "F1 score is defined as the harmonic mean of Precision and Recall\n",
    "The mathematical formula is: F1 score 2(Precisions*Recall)/(Precision+ Recall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9a45a9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "Specificity or True Negative Rate\n",
    "This represents how specific is the model while predicting the True Negatives. Mathematically.\n",
    "Specificity=TN/(TN+FP)\n",
    "Or, it can be said that it quantifies the total number of negatives predicted by the model with respect to the \n",
    "total number of actual negative or non favorable outcomes.\n",
    "Similarly, False Positive rate can be defined as: (1-specificity) Or"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afef76dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "ROC(Receiver Operator Characteristic) ROC is usd to pick the best threshold point\n",
    "We know that the classification algorithms work on the concept of probability of occurrence of the possible outcomes. A probability value lies between 0 and 1.\n",
    "Zero means that there is no probability of occurrence and one means that the occurrence is certain.\n",
    "But while working with real\n",
    "time data, it has been observed that we seldom get a perfect 0 or 1 value. Instead of that, we get different decimal values lying\n",
    "between 0 and 1. Now the question is if we are not getting binary probability values how are we actually determining the class in our classification problem?\n",
    "There comes the concept of Threshold. A threshold is set\n",
    "any probability value below the threshold is a negative outcome, and anything more than the\n",
    "threshold is a favourable or the positive outcome. For Example, if the threshold is 0.5\n",
    "any probability value below 0.5 means a negative or an unfavourable\n",
    "outcome and any value above 0.5 indicates a positive or favourable outcome."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca80f37a",
   "metadata": {},
   "outputs": [],
   "source": [
    "ROC AUC Curve\n",
    "True Positive Rate (TPR) = Out of 100 actual cardio patients how many our model predicted as cardio patient correctly. (Higher the better)\n",
    "False Positive Rate (FPR) = How many people were predicted as cardio petients but in reality they were healthy. (Lower the better)\n",
    "sometimes AUC(Area Under Curve) is used to clearly predict when ROC does not.........\n",
    "Since it is difficult to evaluate the score based on only TPR and FPR data, there comes AUC"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
